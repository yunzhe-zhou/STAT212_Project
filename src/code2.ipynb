{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"code2.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1yk0ctFAOkGyA5iUwwdO6qNz5Wy2T1GBo","authorship_tag":"ABX9TyPNYFhn6q7FRrFzhfojlZD9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"EZn4PoltQXRZ","executionInfo":{"status":"ok","timestamp":1617614078980,"user_tz":-480,"elapsed":657,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["import os\n","os.getcwd()\n","os.chdir(\"drive/My Drive/STAT212/DeepZip_code/src\")"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"HLQN9I10QavU","executionInfo":{"status":"ok","timestamp":1617614080752,"user_tz":-480,"elapsed":1548,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import OneHotEncoder\n","import numpy as np\n","import argparse\n","import contextlib\n","import arithmeticcoding_fast\n","import json\n","from tqdm import tqdm\n","import struct\n","import tempfile\n","import shutil\n","import torch\n","from torch import nn\n","from torch.utils.data import TensorDataset, DataLoader\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F\n","import numpy as np\n","import torch.quantization\n","import zipfile\n","\n","# Device configuration\n","# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device = torch.device('cpu')\n","torch.manual_seed(1)\n","\n","parser = argparse.ArgumentParser(description='Input')\n","parser.add_argument('-model', action='store', dest='model_weights_file',\n","                    help='model file')\n","parser.add_argument('-model_name', action='store', dest='model_name',\n","                    help='model file')\n","parser.add_argument('-batch_size', action='store', dest='batch_size', type=int,\n","                    help='model file')\n","parser.add_argument('-data', action='store', dest='sequence_npy_file',\n","                    help='data file')\n","parser.add_argument('-data_params', action='store', dest='params_file',\n","                    help='params file')\n","parser.add_argument('-output', action='store',dest='output_file_prefix',\n","                    help='compressed file name')\n","\n","args, unknown = parser.parse_known_args()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"rttWm8G0Q1kt","executionInfo":{"status":"ok","timestamp":1617614081657,"user_tz":-480,"elapsed":899,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["def strided_app(a, L, S):  # Window len = L, Stride len/stepsize = S\n","    nrows = ((a.size - L) // S) + 1\n","    n = a.strides[0]\n","    return np.lib.stride_tricks.as_strided(\n","        a, shape=(nrows, L), strides=(S * n, n), writeable=False)\n","    \n","def predict_lstm(X, y, y_original, timesteps, bs, alphabet_size, model_name, final_step=False):       \n","        if not final_step:\n","                num_iters = int((len(X)+timesteps)/bs)\n","                ind = np.array(range(bs))*num_iters\n","                \n","                # open compressed files and compress first few characters using\n","                # uniform distribution\n","                f = [open(args.temp_file_prefix+'.'+str(i),'wb') for i in range(bs)]\n","                bitout = [arithmeticcoding_fast.BitOutputStream(f[i]) for i in range(bs)]\n","                enc = [arithmeticcoding_fast.ArithmeticEncoder(32, bitout[i]) for i in range(bs)]\n","                prob = np.ones(alphabet_size)/alphabet_size\n","                cumul = np.zeros(alphabet_size+1, dtype = np.uint64)\n","                cumul[1:] = np.cumsum(prob*10000000 + 1)        \n","                for i in range(bs):\n","                        for j in range(min(timesteps, num_iters)):\n","                                enc[i].write(cumul, X[ind[i],j])\n","                cumul = np.zeros((bs, alphabet_size+1), dtype = np.uint64)\n","                for j in (range(num_iters - timesteps)):\n","                        x=torch.Tensor(X[ind,:])\n","                        x = x.reshape(-1,timesteps, input_size).to(device)\n","                        outputs = model(x)\n","                        prob=F.softmax(outputs).data.cpu().numpy()\n","                        cumul[:,1:] = np.cumsum(prob*10000000 + 1, axis = 1)\n","                        for i in range(bs):\n","                                enc[i].write(cumul[i,:], y_original[ind[i]])\n","                        ind = ind + 1\n","                # close files\n","                for i in range(bs):\n","                        enc[i].finish()\n","                        bitout[i].close()\n","                        f[i].close()            \n","        else:\n","                f = open(args.temp_file_prefix+'.last','wb')\n","                bitout = arithmeticcoding_fast.BitOutputStream(f)\n","                enc = arithmeticcoding_fast.ArithmeticEncoder(32, bitout)\n","                prob = np.ones(alphabet_size)/alphabet_size\n","                cumul = np.zeros(alphabet_size+1, dtype = np.uint64)\n","                cumul[1:] = np.cumsum(prob*10000000 + 1)        \n","\n","                for j in range(timesteps):\n","                        enc.write(cumul, X[0,j])\n","                for i in (range(len(X))):\n","                        x=torch.Tensor(X[i,:])\n","                        x = x.reshape(-1,timesteps, input_size).to(device)\n","                        outputs = model(x)\n","                        prob=F.softmax(outputs).data.cpu().numpy()\n","                        cumul[1:] = np.cumsum(prob*10000000 + 1)\n","                        enc.write(cumul, y_original[i][0])\n","                enc.finish()\n","                bitout.close()\n","                f.close()\n","        return\n","\n","\n","# variable length integer encoding http://www.codecodex.com/wiki/Variable-Length_Integers\n","def var_int_encode(byte_str_len, f):\n","        while True:\n","                this_byte = byte_str_len&127\n","                byte_str_len >>= 7\n","                if byte_str_len == 0:\n","                        f.write(struct.pack('B',this_byte))\n","                        break\n","                f.write(struct.pack('B',this_byte|128))\n","                byte_str_len -= 1"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"90XBSB6zPfrf","executionInfo":{"status":"ok","timestamp":1617614083543,"user_tz":-480,"elapsed":602,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["file_path=\"lstm_weights\"; quantization=\"no\"\n","# file_path=\"lstm_pruned_weights\"; quantization=\"no\"\n","# file_path=\"lstm_quantization_weights\"; quantization=\"yes\"\n","# file_path=\"lstm_quantization_pruned_weights\"; quantization=\"yes\"\n","\n","args.temp_dir = tempfile.mkdtemp()\n","args.temp_file_prefix = args.temp_dir + \"/compressed\"\n","args.sequence_npy_file=\"../data/processed_files/text8_sub100000.npy\"\n","args.params_file=\"../data/processed_files/text8.param.json\"\n","\n","args.model_weights_file=\"../data/trained_models/text8/\"+file_path\n","args.model_name=\"LSTM\"\n","args.output_file_prefix=\"../data/compressed/text8/\"+file_path+\".compressed\"\n","\n","args.batch_size=10"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"_5M_exb3Pfva","executionInfo":{"status":"ok","timestamp":1617614084967,"user_tz":-480,"elapsed":706,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["# load the data\n","np.random.seed(0)\n","\n","series = np.load(args.sequence_npy_file)\n","series = series.reshape(-1, 1)\n","\n","onehot_encoder = OneHotEncoder(sparse=False)\n","onehot_encoded = onehot_encoder.fit(series)\n","\n","batch_size = args.batch_size\n","timesteps = 64\n","\n","with open(args.params_file, 'r') as f:\n","        params = json.load(f)\n","\n","params['len_series'] = len(series)\n","params['bs'] = batch_size\n","params['timesteps'] = timesteps\n","\n","with open(args.output_file_prefix+'.params','w') as f:\n","        json.dump(params, f, indent=4)\n","\n","alphabet_size = len(params['id2char_dict'])\n","\n","series = series.reshape(-1)\n","data = strided_app(series, timesteps+1, 1)\n","\n","X = data[:, :-1]\n","Y_original = data[:, -1:]\n","Y = onehot_encoder.transform(Y_original)\n","\n","l = int(len(series)/batch_size)*batch_size"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"mxki3UePQZAh","executionInfo":{"status":"ok","timestamp":1617614086242,"user_tz":-480,"elapsed":605,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["# Hyper Parameters\n","num_epochs=10            \n","input_size = 1   \n","hidden_size = 64\n","num_layers = 2\n","num_classes = alphabet_size\n","lr = 0.01   \n","\n","\n","# Define LSTM model\n","class simpleLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super(simpleLSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True).to(device)\n","        self.fc = nn.Linear(hidden_size, num_classes).to(device)\n","\n","    def forward(self, x):\n","        # initialize\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","        # forward propagate lstm\n","        out, (h_n, h_c) = self.lstm(x, (h0, c0))\n","\n","        # output\n","        out =self.fc(out[:, -1, :])\n","        return out\n","\n","model = simpleLSTM(input_size, hidden_size, num_layers, num_classes)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ycd_qtmQwE2","executionInfo":{"status":"ok","timestamp":1617614087435,"user_tz":-480,"elapsed":502,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["# unzip the compressed models\n","zip_path=\"../data/trained_models/text8/\"+file_path+ \".zip\"\n","save_path=\"../data/trained_models/text8\"\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(save_path)\n","\n","if quantization==\"yes\":\n","    # load the quantized model weights\n","    model = torch.quantization.quantize_dynamic(\n","        model, {nn.LSTM, nn.Linear}, dtype=torch.qint8\n","    )\n","    model.load_state_dict(torch.load(args.model_weights_file))\n","else:\n","    #load the model weights\n","    model.load_state_dict(torch.load(args.model_weights_file))"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nPYzdx2ZPfyR","executionInfo":{"status":"ok","timestamp":1617614157461,"user_tz":-480,"elapsed":69327,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}},"outputId":"38ad2fbd-f385-4636-e543-29b67c813302"},"source":["# compress the data \n","predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, args.model_name)\n","\n","if l < len(series)-timesteps:\n","        predict_lstm(X[l:,:], Y[l:,:], Y_original[l:], timesteps, 1, alphabet_size, args.model_name, final_step = True)\n","else:\n","        f = open(args.temp_file_prefix+'.last','wb')\n","        bitout = arithmeticcoding_fast.BitOutputStream(f)\n","        enc = arithmeticcoding_fast.ArithmeticEncoder(32, bitout) \n","        prob = np.ones(alphabet_size)/alphabet_size\n","        \n","        cumul = np.zeros(alphabet_size+1, dtype = np.uint64)\n","        cumul[1:] = np.cumsum(prob*10000000 + 1)        \n","        for j in range(l, len(series)):\n","                enc.write(cumul, series[j])\n","        enc.finish()\n","        bitout.close() \n","        f.close()\n","\n","# combine files into one file\n","f = open(args.output_file_prefix+'.combined','wb')\n","for i in range(batch_size):\n","        f_in = open(args.temp_file_prefix+'.'+str(i),'rb')\n","        byte_str = f_in.read()\n","        byte_str_len = len(byte_str)\n","        var_int_encode(byte_str_len, f)\n","        f.write(byte_str)\n","        f_in.close()\n","f_in = open(args.temp_file_prefix+'.last','rb')\n","byte_str = f_in.read()\n","byte_str_len = len(byte_str)\n","var_int_encode(byte_str_len, f)\n","f.write(byte_str)\n","f_in.close()\n","f.close()\n","shutil.rmtree(args.temp_dir)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GYFWXILnJMAE","executionInfo":{"status":"ok","timestamp":1617614161580,"user_tz":-480,"elapsed":722,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}},"outputId":"4a71b658-1088-464f-f11f-f0fd93fd098a"},"source":["compressed_path='../data/files_to_be_compressed/text8/text8_sub'+str(len(series))+'.gz'\n","size=os.path.getsize(compressed_path)/1024\n","print(\"Size of compressed data in Gzip: %.2f KB\" % size)\n","\n","print(\"##############################################################\")\n","compressed_path=\"../data/compressed/text8/lstm_weights.compressed.combined\"\n","size=os.path.getsize(compressed_path)/1024\n","print(\"Size of compressed data in Deepzip with baseline model: %.2f KB\" % size)\n","\n","compressed_path='../data/trained_models/text8/lstm_weights.zip'\n","size=os.path.getsize(compressed_path)/1024\n","print(\"Size of compressed baseline model: %.2f KB\" % size)\n","\n","print(\"##############################################################\")\n","compressed_path=\"../data/compressed/text8/lstm_pruned_weights.compressed.combined\"\n","size=os.path.getsize(compressed_path)/1024\n","print(\"Size of compressed data in Deepzip with pruned model: %.2f KB\" % size)\n","\n","compressed_path='../data/trained_models/text8/lstm_pruned_weights.zip'\n","size=os.path.getsize(compressed_path)/1024\n","print(\"Size of compressed pruned model: %.2f KB\" % size)\n","\n","print(\"##############################################################\")\n","compressed_path=\"../data/compressed/text8/lstm_quantization_weights.compressed.combined\"\n","size=os.path.getsize(compressed_path)/1024\n","print(\"Size of compressed data in Deepzip with quantized model: %.2f KB\" % size)\n","\n","compressed_path='../data/trained_models/text8/lstm_quantization_weights.zip'\n","size=os.path.getsize(compressed_path)/1024\n","print(\"Size of compressed quantized model: %.2f KB\" % size)\n","\n","print(\"##############################################################\")\n","compressed_path=\"../data/compressed/text8/lstm_quantization_pruned_weights.compressed.combined\"\n","size=os.path.getsize(compressed_path)/1024\n","print(\"Size of compressed data in Deepzip with quantization and pruning: %.2f KB\" % size)\n","\n","compressed_path='../data/trained_models/text8/lstm_quantization_pruned_weights.zip'\n","size=os.path.getsize(compressed_path)/1024\n","print(\"Size of compressed quantized and pruned model: %.2f KB\" % size)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Size of compressed data in Gzip: 32.45 KB\n","##############################################################\n","Size of compressed data in Deepzip with baseline model: 28.39 KB\n","Size of compressed baseline model: 192.23 KB\n","##############################################################\n","Size of compressed data in Deepzip with pruned model: 31.35 KB\n","Size of compressed pruned model: 182.52 KB\n","##############################################################\n","Size of compressed data in Deepzip with quantized model: 28.71 KB\n","Size of compressed quantized model: 44.95 KB\n","##############################################################\n","Size of compressed data in Deepzip with quantization and pruning: 31.70 KB\n","Size of compressed quantized and pruned model: 44.42 KB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fHWjW5fdFVgr"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cggm4xnODGFh"},"source":[""],"execution_count":null,"outputs":[]}]}