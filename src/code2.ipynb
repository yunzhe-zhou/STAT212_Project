{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"code21_revise.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1V-vslzzRuDLygubSoV81KBFAMD8r62FP","authorship_tag":"ABX9TyODSDWj18cBVMkhyiiWszxY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"EZn4PoltQXRZ","executionInfo":{"status":"ok","timestamp":1617969197800,"user_tz":-480,"elapsed":905,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["import os\n","os.getcwd()\n","os.chdir(\"drive/My Drive/STAT212/DeepZip_code/src\")"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"HLQN9I10QavU","executionInfo":{"status":"ok","timestamp":1617969202446,"user_tz":-480,"elapsed":5542,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import OneHotEncoder\n","import numpy as np\n","import argparse\n","import contextlib\n","import arithmeticcoding_fast\n","import json\n","from tqdm import tqdm\n","import struct\n","import tempfile\n","import shutil\n","import torch\n","from torch import nn\n","from torch.utils.data import TensorDataset, DataLoader\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F\n","import numpy as np\n","import torch.quantization\n","import zipfile\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# device = torch.device('cpu')\n","torch.manual_seed(1)\n","\n","parser = argparse.ArgumentParser(description='Input')\n","parser.add_argument('-model', action='store', dest='model_weights_file',\n","                    help='model file')\n","parser.add_argument('-model_name', action='store', dest='model_name',\n","                    help='model file')\n","parser.add_argument('-batch_size', action='store', dest='batch_size', type=int,\n","                    help='model file')\n","parser.add_argument('-data', action='store', dest='sequence_npy_file',\n","                    help='data file')\n","parser.add_argument('-data_params', action='store', dest='params_file',\n","                    help='params file')\n","parser.add_argument('-output', action='store',dest='output_file_prefix',\n","                    help='compressed file name')\n","\n","args, unknown = parser.parse_known_args()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"rttWm8G0Q1kt","executionInfo":{"status":"ok","timestamp":1617969202447,"user_tz":-480,"elapsed":5536,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["def strided_app(a, L, S):  # Window len = L, Stride len/stepsize = S\n","    nrows = ((a.size - L) // S) + 1\n","    n = a.strides[0]\n","    return np.lib.stride_tricks.as_strided(\n","        a, shape=(nrows, L), strides=(S * n, n), writeable=False)\n","    \n","def predict_lstm(X, y, y_original, timesteps, bs, alphabet_size, model_name, final_step=False):       \n","        if not final_step:\n","                num_iters = int((len(X)+timesteps)/bs)\n","                ind = np.array(range(bs))*num_iters\n","                \n","                # open compressed files and compress first few characters using\n","                # uniform distribution\n","                f = [open(args.temp_file_prefix+'.'+str(i),'wb') for i in range(bs)]\n","                bitout = [arithmeticcoding_fast.BitOutputStream(f[i]) for i in range(bs)]\n","                enc = [arithmeticcoding_fast.ArithmeticEncoder(32, bitout[i]) for i in range(bs)]\n","                prob = np.ones(alphabet_size)/alphabet_size\n","                cumul = np.zeros(alphabet_size+1, dtype = np.uint64)\n","                cumul[1:] = np.cumsum(prob*10000000 + 1)        \n","                for i in range(bs):\n","                        for j in range(min(timesteps, num_iters)):\n","                                enc[i].write(cumul, X[ind[i],j])\n","                cumul = np.zeros((bs, alphabet_size+1), dtype = np.uint64)\n","                for j in (range(num_iters - timesteps)):\n","                        x=torch.Tensor(X[ind,:])\n","                        x = x.reshape(-1,timesteps, input_size).to(device)\n","                        outputs = model(x)\n","                        prob=F.softmax(outputs).data.cpu().numpy()\n","                        cumul[:,1:] = np.cumsum(prob*10000000 + 1, axis = 1)\n","                        for i in range(bs):\n","                                enc[i].write(cumul[i,:], y_original[ind[i]])\n","                        ind = ind + 1\n","                # close files\n","                for i in range(bs):\n","                        enc[i].finish()\n","                        bitout[i].close()\n","                        f[i].close()            \n","        else:\n","                f = open(args.temp_file_prefix+'.last','wb')\n","                bitout = arithmeticcoding_fast.BitOutputStream(f)\n","                enc = arithmeticcoding_fast.ArithmeticEncoder(32, bitout)\n","                prob = np.ones(alphabet_size)/alphabet_size\n","                cumul = np.zeros(alphabet_size+1, dtype = np.uint64)\n","                cumul[1:] = np.cumsum(prob*10000000 + 1)        \n","\n","                for j in range(timesteps):\n","                        enc.write(cumul, X[0,j])\n","                for i in (range(len(X))):\n","                        x=torch.Tensor(X[i,:])\n","                        x = x.reshape(-1,timesteps, input_size).to(device)\n","                        outputs = model(x)\n","                        prob=F.softmax(outputs).data.cpu().numpy()\n","                        cumul[1:] = np.cumsum(prob*10000000 + 1)\n","                        enc.write(cumul, y_original[i][0])\n","                enc.finish()\n","                bitout.close()\n","                f.close()\n","        return\n","\n","\n","# variable length integer encoding http://www.codecodex.com/wiki/Variable-Length_Integers\n","def var_int_encode(byte_str_len, f):\n","        while True:\n","                this_byte = byte_str_len&127\n","                byte_str_len >>= 7\n","                if byte_str_len == 0:\n","                        f.write(struct.pack('B',this_byte))\n","                        break\n","                f.write(struct.pack('B',this_byte|128))\n","                byte_str_len -= 1"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"90XBSB6zPfrf","executionInfo":{"status":"ok","timestamp":1617969202448,"user_tz":-480,"elapsed":5531,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["file_path=\"lstm_weights\"; quantization=\"no\"\n","# file_path=\"lstm_pruned_weights\"; quantization=\"no\"\n","# file_path=\"lstm_quantization_weights\"; quantization=\"yes\"\n","# file_path=\"lstm_quantization_pruned_weights\"; quantization=\"yes\"\n","\n","args.temp_dir = tempfile.mkdtemp()\n","args.temp_file_prefix = args.temp_dir + \"/compressed\"\n","args.sequence_npy_file=\"../data/processed_files/text8_sub2000000.npy\"\n","args.params_file=\"../data/processed_files/text8.param.json\"\n","\n","args.model_weights_file=\"../data/trained_models/text8/\"+file_path\n","args.model_name=\"LSTM\"\n","args.output_file_prefix=\"../data/compressed/text8/\"+file_path+\".compressed\""],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"_5M_exb3Pfva","executionInfo":{"status":"ok","timestamp":1617969207514,"user_tz":-480,"elapsed":10594,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["# load the data\n","np.random.seed(0)\n","\n","series = np.load(args.sequence_npy_file)\n","series = series.reshape(-1, 1)\n","\n","onehot_encoder = OneHotEncoder(sparse=False)\n","onehot_encoded = onehot_encoder.fit(series)\n","\n","args.batch_size=int(len(series)/10000)\n","batch_size = args.batch_size\n","timesteps = 64\n","\n","with open(args.params_file, 'r') as f:\n","        params = json.load(f)\n","\n","params['len_series'] = len(series)\n","params['bs'] = batch_size\n","params['timesteps'] = timesteps\n","\n","with open(args.output_file_prefix+'.params','w') as f:\n","        json.dump(params, f, indent=4)\n","\n","alphabet_size = len(params['id2char_dict'])\n","\n","series = series.reshape(-1)\n","data = strided_app(series, timesteps+1, 1)\n","\n","X = data[:, :-1]\n","Y_original = data[:, -1:]\n","Y = onehot_encoder.transform(Y_original)\n","\n","l = int(len(series)/batch_size)*batch_size"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"868Jnr3vqRK-","executionInfo":{"status":"ok","timestamp":1617969207515,"user_tz":-480,"elapsed":10590,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["class LSTM(nn.Module):\n","    def __init__(self, num_classes, hidden_size1=32, hidden_size2=32, num_layers=2):\n","        super(LSTM, self).__init__()\n","        self.hidden_size1 = hidden_size1\n","        self.hidden_size2 = hidden_size2\n","        self.num_layers = num_layers\n","        self.embedding= nn.Embedding(num_classes, 32).to(device)\n","        self.lstm = nn.LSTM(32, hidden_size1, num_layers, batch_first=True).to(device)\n","        self.fc1 = nn.Linear(hidden_size1, hidden_size2).to(device)\n","        self.fc2 = nn.Linear(hidden_size2, num_classes).to(device)\n","        \n","\n","    def forward(self, x):\n","        # initialize\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size1).to(device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size1).to(device)\n","        \n","        out=self.embedding(x[:,:,0].long())\n","        out, (h_n, h_c) = self.lstm(out, (h0, c0))\n","        out =self.fc1(out[:, -1, :])\n","        out =self.fc2(nn.ReLU()(out))\n","\n","        return out\n","\n","\n","class GRU(nn.Module):\n","    def __init__(self, num_classes, hidden_size1=32, hidden_size2=32, num_layers=2):\n","        super(GRU, self).__init__()\n","        self.hidden_size1 = hidden_size1\n","        self.hidden_size2 = hidden_size2\n","        self.num_layers = num_layers\n","        self.embedding= nn.Embedding(num_classes, 32).to(device)\n","        self.lstm = nn.GRU(32, hidden_size1, num_layers, batch_first=True).to(device)\n","        self.fc1 = nn.Linear(hidden_size1, hidden_size2).to(device)\n","        self.fc2 = nn.Linear(hidden_size2, num_classes).to(device)\n","        \n","\n","    def forward(self, x):\n","        # initialize\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size1).to(device)\n","        \n","        out=self.embedding(x[:,:,0].long())\n","        out, _ = self.lstm(out, h0)\n","        out =self.fc1(out[:, -1, :])\n","        out =self.fc2(nn.ReLU()(out))\n","\n","        return out\n","\n","class biLSTM(nn.Module):\n","    def __init__(self, num_classes, hidden_size1=32, hidden_size2=32, num_layers=2):\n","        super(biLSTM, self).__init__()\n","        self.hidden_size1 = hidden_size1\n","        self.hidden_size2 = hidden_size2\n","        self.num_layers = num_layers\n","        self.embedding= nn.Embedding(num_classes, 32).to(device)\n","        self.lstm = nn.LSTM(32, hidden_size1, num_layers, batch_first=True,bidirectional=True).to(device)\n","        self.fc1 = nn.Linear(hidden_size1*2, hidden_size2*2).to(device)\n","        self.fc2 = nn.Linear(hidden_size2*2, num_classes).to(device)\n","        \n","\n","    def forward(self, x):\n","        # initialize\n","        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size1).to(device)\n","        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size1).to(device)\n","        \n","        out=self.embedding(x[:,:,0].long())\n","        out, (h_n, h_c) = self.lstm(out, (h0, c0))\n","        out =self.fc1(out[:, -1, :])\n","        out =self.fc2(nn.ReLU()(out))\n","\n","        return out\n","        \n","class biGRU(nn.Module):\n","    def __init__(self, num_classes, hidden_size1=32, hidden_size2=32, num_layers=2):\n","        super(biGRU, self).__init__()\n","        self.hidden_size1 = hidden_size1\n","        self.hidden_size2 = hidden_size2\n","        self.num_layers = num_layers\n","        self.embedding= nn.Embedding(num_classes, 32).to(device)\n","        self.lstm = nn.GRU(32, hidden_size1, num_layers, batch_first=True,bidirectional=True).to(device)\n","        self.fc1 = nn.Linear(hidden_size1*2, hidden_size2*2).to(device)\n","        self.fc2 = nn.Linear(hidden_size2*2, num_classes).to(device)\n","        \n","\n","    def forward(self, x):\n","        # initialize\n","        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size1).to(device)\n","        \n","        out=self.embedding(x[:,:,0].long())\n","        out, _ = self.lstm(out, h0)\n","        out =self.fc1(out[:, -1, :])\n","        out =self.fc2(nn.ReLU()(out))\n","\n","        return out"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"mxki3UePQZAh","executionInfo":{"status":"ok","timestamp":1617969213177,"user_tz":-480,"elapsed":16248,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["# Hyper Parameters       \n","input_size = 1   \n","num_epochs=10          \n","hidden_size1 = 32\n","hidden_size2 = 32\n","num_layers = 2\n","num_classes = alphabet_size\n","lr = 0.01 \n","\n","model = LSTM(num_classes,hidden_size1, hidden_size2, num_layers)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ycd_qtmQwE2","executionInfo":{"status":"ok","timestamp":1617969216096,"user_tz":-480,"elapsed":19164,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["# unzip the compressed models\n","zip_path=\"../data/trained_models/text8/\"+file_path+ \".zip\"\n","save_path=\"../data/trained_models/text8\"\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(save_path)\n","\n","model=model.to('cpu')\n","if quantization==\"yes\":\n","    # load the quantized model weights\n","    model = torch.quantization.quantize_dynamic(\n","        model, {nn.LSTM, nn.Linear}, dtype=torch.qint8\n","    )\n","    model.load_state_dict(torch.load(args.model_weights_file))\n","else:\n","    #load the model weights\n","    model.load_state_dict(torch.load(args.model_weights_file))\n","model=model.to(device)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nPYzdx2ZPfyR","executionInfo":{"status":"ok","timestamp":1617969274795,"user_tz":-480,"elapsed":77859,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}},"outputId":"5dbf200d-c452-41d6-fb7c-8985e498b125"},"source":["# compress the data \n","predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, args.model_name)\n","\n","if l < len(series)-timesteps:\n","        predict_lstm(X[l:,:], Y[l:,:], Y_original[l:], timesteps, 1, alphabet_size, args.model_name, final_step = True)\n","else:\n","        f = open(args.temp_file_prefix+'.last','wb')\n","        bitout = arithmeticcoding_fast.BitOutputStream(f)\n","        enc = arithmeticcoding_fast.ArithmeticEncoder(32, bitout) \n","        prob = np.ones(alphabet_size)/alphabet_size\n","        \n","        cumul = np.zeros(alphabet_size+1, dtype = np.uint64)\n","        cumul[1:] = np.cumsum(prob*10000000 + 1)        \n","        for j in range(l, len(series)):\n","                enc.write(cumul, series[j])\n","        enc.finish()\n","        bitout.close() \n","        f.close()\n","\n","# combine files into one file\n","f = open(args.output_file_prefix+'.combined','wb')\n","for i in range(batch_size):\n","        f_in = open(args.temp_file_prefix+'.'+str(i),'rb')\n","        byte_str = f_in.read()\n","        byte_str_len = len(byte_str)\n","        var_int_encode(byte_str_len, f)\n","        f.write(byte_str)\n","        f_in.close()\n","f_in = open(args.temp_file_prefix+'.last','rb')\n","byte_str = f_in.read()\n","byte_str_len = len(byte_str)\n","var_int_encode(byte_str_len, f)\n","f.write(byte_str)\n","f_in.close()\n","f.close()\n","shutil.rmtree(args.temp_dir)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GYFWXILnJMAE","executionInfo":{"status":"ok","timestamp":1617969276054,"user_tz":-480,"elapsed":79113,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}},"outputId":"6ba16f33-a00d-4437-95db-f22ea7ca380a"},"source":["compressed_path='../data/files_to_be_compressed/text8/text8_sub'+str(len(series))+'.gz'\n","size=os.path.getsize(compressed_path)/1024\n","print(\"Size of compressed data in Gzip: %.2f KB\" % size)\n","\n","print(\"##############################################################\")\n","compressed_path=\"../data/compressed/text8/lstm_weights.compressed.combined\"\n","size=os.path.getsize(compressed_path)/1024\n","print(\"Size of compressed data in Deepzip with baseline model: %.2f KB\" % size)\n","\n","compressed_path='../data/trained_models/text8/lstm_weights.zip'\n","size=os.path.getsize(compressed_path)/1024\n","print(\"Size of compressed baseline model: %.2f KB\" % size)\n","\n","print(\"##############################################################\")\n","compressed_path=\"../data/compressed/text8/lstm_pruned_weights.compressed.combined\"\n","size=os.path.getsize(compressed_path)/1024\n","print(\"Size of compressed data in Deepzip with pruned model: %.2f KB\" % size)\n","\n","compressed_path='../data/trained_models/text8/lstm_pruned_weights.zip'\n","size=os.path.getsize(compressed_path)/1024\n","print(\"Size of compressed pruned model: %.2f KB\" % size)\n","\n","print(\"##############################################################\")\n","compressed_path=\"../data/compressed/text8/lstm_quantization_weights.compressed.combined\"\n","size=os.path.getsize(compressed_path)/1024\n","print(\"Size of compressed data in Deepzip with quantized model: %.2f KB\" % size)\n","\n","compressed_path='../data/trained_models/text8/lstm_quantization_weights.zip'\n","size=os.path.getsize(compressed_path)/1024\n","print(\"Size of compressed quantized model: %.2f KB\" % size)\n","\n","print(\"##############################################################\")\n","compressed_path=\"../data/compressed/text8/lstm_quantization_pruned_weights.compressed.combined\"\n","size=os.path.getsize(compressed_path)/1024\n","print(\"Size of compressed data in Deepzip with quantization and pruning: %.2f KB\" % size)\n","\n","compressed_path='../data/trained_models/text8/lstm_quantization_pruned_weights.zip'\n","size=os.path.getsize(compressed_path)/1024\n","print(\"Size of compressed quantized and pruned model: %.2f KB\" % size)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Size of compressed data in Gzip: 642.98 KB\n","##############################################################\n","Size of compressed data in Deepzip with baseline model: 542.63 KB\n","Size of compressed baseline model: 73.02 KB\n","##############################################################\n","Size of compressed data in Deepzip with pruned model: 320.11 KB\n","Size of compressed pruned model: 67.45 KB\n","##############################################################\n","Size of compressed data in Deepzip with quantized model: 276.17 KB\n","Size of compressed quantized model: 23.83 KB\n","##############################################################\n","Size of compressed data in Deepzip with quantization and pruning: 321.07 KB\n","Size of compressed quantized and pruned model: 23.13 KB\n"],"name":"stdout"}]}]}