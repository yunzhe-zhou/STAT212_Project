{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"code1.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"wXlF4dbFLiTk","executionInfo":{"status":"ok","timestamp":1617352334870,"user_tz":-480,"elapsed":1327,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["import os\n","os.getcwd()\n","os.chdir(\"drive/My Drive/STAT212/DeepZip_code/src\")"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"jJo3EIFrLqCW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617352337841,"user_tz":-480,"elapsed":2182,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}},"outputId":"16613ae2-8125-41e9-b0ca-7f126880e918"},"source":["from sklearn.preprocessing import OneHotEncoder\n","import numpy as np\n","import torch\n","from torch import nn\n","from torch.utils.data import TensorDataset, DataLoader\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F\n","import torch.nn.utils.prune as prune\n","import torch.quantization\n","import copy \n","import os\n","import zipfile\n","import tempfile\n","import shutil\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# device = torch.device('cpu')\n","torch.manual_seed(1)"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7faf76ed34d0>"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"modH9c2fMRy9","executionInfo":{"status":"ok","timestamp":1617352339250,"user_tz":-480,"elapsed":689,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["file_path=\"../data/processed_files/text8.npy\"\n","batch_size=128\n","time_steps= 64"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"lrf_lUN-MumG","executionInfo":{"status":"ok","timestamp":1617352340982,"user_tz":-480,"elapsed":749,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["def strided_app(a, L, S):  # Window len = L, Stride len/stepsize = S\n","        nrows = ((a.size - L) // S) + 1\n","        n = a.strides[0]\n","        return np.lib.stride_tricks.as_strided(a, shape=(nrows, L), strides=(S * n, n), writeable=False)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"61nFwuOAVnWY","executionInfo":{"status":"ok","timestamp":1617352476120,"user_tz":-480,"elapsed":3919,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["# load the preprocessed data\n","series = np.load(file_path)\n","series = series.reshape(-1, 1)\n","series=series[0:100000]\n","\n","onehot_encoder = OneHotEncoder(sparse=False)\n","onehot_encoded = onehot_encoder.fit(series)\n","series = series.reshape(-1)\n","\n","data = strided_app(series, time_steps+1, 1)\n","l = int(len(data)/batch_size) * batch_size\n","\n","data = data[:l] \n","X = data[:, :-1]\n","Y = data[:, -1:].reshape([-1,])\n","Y_hot = onehot_encoder.transform(data[:, -1:])"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZHpei-0bMy_H","executionInfo":{"status":"ok","timestamp":1617352479612,"user_tz":-480,"elapsed":883,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["# Hyper Parameters\n","num_epochs=10           \n","input_size = 1   \n","hidden_size = 64\n","num_layers = 1\n","num_classes = Y_hot.shape[1]\n","lr = 0.01   \n","\n","\n","# Define LSTM model\n","class simpleLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super(simpleLSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True).to(device)\n","        self.fc = nn.Linear(hidden_size, num_classes).to(device)\n","\n","    def forward(self, x):\n","        # initialize\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","        # forward propagate lstm\n","        out, (h_n, h_c) = self.lstm(x, (h0, c0))\n","\n","        # output\n","        out =self.fc(out[:, -1, :])\n","        return out\n","\n","model = simpleLSTM(input_size, hidden_size, num_layers, num_classes)\n","\n","# loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"vp2OMWdRMzBY"},"source":["# load training data\n","train_data = TensorDataset(torch.Tensor(X),torch.Tensor(Y).long())\n","train_loader = DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True) \n","\n","# train\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (x, y) in enumerate(train_loader):\n","        x = x.reshape(-1, time_steps, input_size).to(device)\n","        y = y.to(device)\n","\n","        # forward pass\n","        outputs = model(x)\n","        loss = criterion(outputs, y)\n","\n","        # backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if i % 100 == 0:\n","            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n","                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HtnnUrz-Nyyx","executionInfo":{"status":"ok","timestamp":1617352462234,"user_tz":-480,"elapsed":9896,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}},"outputId":"91a2b9a8-4959-4a2c-9e80-4cd354344ffb"},"source":["# test accuary\n","test_data = TensorDataset(torch.Tensor(X),torch.Tensor(Y).long())\n","test_loader = DataLoader(dataset=test_data,batch_size=batch_size,shuffle=True) \n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for x, y in test_loader:\n","        x = x.reshape(-1, time_steps, input_size).to(device)\n","        y = y.to(device)\n","        outputs = model(x)\n","        prob=F.softmax(outputs)\n","        _, predicted = torch.max(prob, 1)\n","        total += y.size(0)\n","        correct += (predicted == y).sum().item()\n","\n","    print('Test Accuracy of the model on the 10000 test x: {} %'.format(100 * correct / total))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  # This is added back by InteractiveShellApp.init_path()\n"],"name":"stderr"},{"output_type":"stream","text":["Test Accuracy of the model on the 10000 test x: 46.03461593878074 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P_EQ2IFv8ScT","executionInfo":{"status":"ok","timestamp":1617351940015,"user_tz":-480,"elapsed":1615,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["torch.save(model.state_dict(), \"../data/trained_models/text8/temptory/lstm_weights\")"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"urPSCyCr6KZ7","executionInfo":{"status":"ok","timestamp":1617351940581,"user_tz":-480,"elapsed":1461,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["# copy a model0 for pruning\n","model0=copy.deepcopy(model)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"6HE_GYDqNaI1","executionInfo":{"status":"ok","timestamp":1617351941830,"user_tz":-480,"elapsed":1446,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["for name, module in model0.named_modules():\n","    # prune 20% of connections in all lstm layers\n","    if isinstance(module, torch.nn.LSTM):\n","        prune.l1_unstructured(module, name='weight_hh_l0', amount=0.2)\n","        prune.l1_unstructured(module, name='weight_ih_l0', amount=0.2)\n","    # prune 40% of connections in all linear layers\n","    elif isinstance(module, torch.nn.Linear):\n","        prune.l1_unstructured(module, name='weight', amount=0.4)\n","\n","for name, module in model0.named_modules():\n","    # prune 20% of connections in all lstm layers\n","    if isinstance(module, torch.nn.LSTM):\n","        prune.remove(module, 'weight_hh_l0')\n","        prune.remove(module, 'weight_ih_l0')\n","    # prune 40% of connections in all linear layers\n","    elif isinstance(module, torch.nn.Linear):\n","        prune.remove(module, 'weight')"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"fDup0-i6OFx8","executionInfo":{"status":"ok","timestamp":1617351942426,"user_tz":-480,"elapsed":820,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["torch.save(model0.state_dict(), \"../data/trained_models/text8/temptory/lstm_pruned_weights\")"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"08e4wTcjNaLY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617351943554,"user_tz":-480,"elapsed":1006,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}},"outputId":"c7dba8e8-011d-4c59-ff29-3611975e0192"},"source":["# quantize the unpruned model\n","quantized_model = torch.quantization.quantize_dynamic(\n","    model.to('cpu'), {nn.LSTM, nn.Linear}, dtype=torch.qint8\n",")\n","print(quantized_model)\n","\n","# quantize the pruned model\n","quantized_model0 = torch.quantization.quantize_dynamic(\n","    model0.to('cpu'), {nn.LSTM, nn.Linear}, dtype=torch.qint8\n",")\n","print(quantized_model0)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["simpleLSTM(\n","  (lstm): DynamicQuantizedLSTM(1, 64, batch_first=True)\n","  (fc): DynamicQuantizedLinear(in_features=64, out_features=27, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",")\n","simpleLSTM(\n","  (lstm): DynamicQuantizedLSTM(1, 64, batch_first=True)\n","  (fc): DynamicQuantizedLinear(in_features=64, out_features=27, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s4UW96rsOt-m","executionInfo":{"status":"ok","timestamp":1617351945128,"user_tz":-480,"elapsed":770,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["torch.save(quantized_model.state_dict(), \"../data/trained_models/text8/temptory/lstm_quantization_weights\")\n","torch.save(quantized_model0.state_dict(), \"../data/trained_models/text8/temptory/lstm_quantization_pruned_weights\")"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"EmmVrWuf9mZX","executionInfo":{"status":"ok","timestamp":1617351945996,"user_tz":-480,"elapsed":944,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}}},"source":["# define a gzip function\n","def get_gzipped_model_size(file,path):\n","  # Returns size of gzipped model, in bytes.\n","  _, zipped_file = tempfile.mkstemp('.zip')\n","  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n","    f.write(file)\n","  shutil.copy(zipped_file, path)\n","  return os.path.getsize(zipped_file)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E429fy6pBkoS","executionInfo":{"status":"ok","timestamp":1617351947898,"user_tz":-480,"elapsed":1750,"user":{"displayName":"LAI MZ","photoUrl":"","userId":"15718361365988174990"}},"outputId":"6a809e2b-9c16-4247-edfa-91c9986f575d"},"source":["os.chdir(\"../data/trained_models/text8/temptory\")\n","file_path=\"lstm_weights\"\n","save_path=\"../lstm_weights.zip\"\n","print(\"Size of gzipped baseline model: %.2f bytes\" % (get_gzipped_model_size(file_path,save_path)))\n","\n","file_path=\"lstm_pruned_weights\"\n","save_path=\"../lstm_pruned_weights.zip\"\n","print(\"Size of gzipped pruned model: %.2f bytes\" % (get_gzipped_model_size(file_path,save_path)))\n","\n","file_path=\"lstm_quantization_weights\"\n","save_path=\"../lstm_quantization_weights.zip\"\n","print(\"Size of gzipped quantized baseline model: %.2f bytes\" % (get_gzipped_model_size(file_path,save_path)))\n","\n","file_path=\"lstm_quantization_pruned_weights\"\n","save_path=\"../lstm_quantization_pruned_weights.zip\"\n","print(\"Size of gzipped quantized and pruned model: %.2f bytes\" % (get_gzipped_model_size(file_path,save_path)))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Size of gzipped baseline model: 71837.00 bytes\n","Size of gzipped pruned model: 61241.00 bytes\n","Size of gzipped quantized baseline model: 18921.00 bytes\n","Size of gzipped quantized and pruned model: 17742.00 bytes\n"],"name":"stdout"}]}]}